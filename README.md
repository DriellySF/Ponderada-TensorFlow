# Ponderada-Tranformers

O notebook foi muito útil para entender a importância da lógica por trás dos transformers. Eu tinha consciência do pulo que essa forma de processar é em relação às anteriores, mas não tinha ideia do quão grande foi esse passo. O tutorial ajudou a ter uma ideia mais concreta da camada de atenção e o seu papel em todo processo.
Ainda sim, é um notebook extremamente demorado para rodar, tanto usando CPU quanto GPU, o que ajuda a pensar: se isso já demora com a lógica dos transformers, quanto que não iria demorar usando RNN, que nem parelilazação usa (ou pouco se beneficia)?

As anotações em markdown são muito didáticas e cumprem bem o papel de ajudar a entender o código, assim como as imagens/diagramas foram extremamente úteis para dar forma aos conteúdos explicados, principalmente as camadas de atenção. Além disso, todas as referências (ou a maioria) e links externos no notebook foram de grande ajuda, com uma didática parecida e bem diretas ao ponto; as referências e explicações nas implementações feitas de forma diferente das implementadas no artigo original também foram legais de observar. Ademais, pegar o diagrama original e mostrar em código cada bloco foi essencial para entender a lógica.


